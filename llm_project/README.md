<div>
    
# ğŸ§  MultiAgents LLM Project  
### Local, Modular & Extensible AI Agent Framework

A fully modular Python framework for running, testing, and simulating multiple LLM-powered agents â€” **locally**, **offline**, and **without API keys**.  
Supports:
- Simulated OpenAI & Gemini agents  
- Local HuggingFace models  
- Real local models via **Ollama**  
- Unified async agent interface  

Perfect for **research**, **offline AI development**, **agent prototyping**, and **multi-agent experiments**.

</div>

---

# ğŸ“‚ Folder Structure

```bash
llm_project/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ agent.py               # Core Agent class
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml            # Global configuration (model, paths, params)
â”‚   â””â”€â”€ config_loader.py       # Reads & parses YAML config
â”‚
â”œâ”€â”€ llm_providers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py                # Base provider interface
â”‚   â”œâ”€â”€ openai_provider.py     # Simulated OpenAI client
â”‚   â”œâ”€â”€ gemini_provider.py     # Simulated Gemini client
â”‚   â”œâ”€â”€ huggingface_provider.py # Local offline HF model
â”‚   â”œâ”€â”€ ollama_provider.py     # Real local Ollama inference
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ logger.py              # Global logging utility
â”‚
â”œâ”€â”€ main.py                    # Entry point to run agents
â”œâ”€â”€ README.md                  # Documentation
â””â”€â”€ requirements.txt           # Dependencies
```

---

# ğŸ’¡ Key Features

### ğŸ”· 1. Simulated Agents (Offline, No API Keys Needed)
- **OpenAI Agent (Simulated)**
- **Gemini Agent (Simulated)**  
Both generate realistic mock responses **without internet**.

### ğŸŸ¡ 2. Offline HuggingFace Models
Run LLMs (like **GPT-2**) using `transformers`.  
Ideal for offline experiments & local inference.

### ğŸŸ¢ 3. Real Local Models via Ollama
Supports **actual LLM inference** using:
```bash
ollama run tinyllama
```

### âš™ï¸ 4. One Unified Async Interface
All agents follow the same function pattern:
```python
await agent.generate_response("hello")
```

### ğŸš€ 5. Modular & Extensible
Add new agents by:
- extending the base client  
- adding a function  
- updating the demo  

---

# ğŸ§ª Demo Overview (`demo/run_demo.py`)

The demo runs all agents sequentially:

### ğŸ”§ Demo functions
```python
async def demo_openai()         # Simulated OpenAI agent
async def demo_gemini()         # Simulated Gemini agent
async def demo_huggingface()    # Offline HuggingFace model
async def demo_ollama()         # Real local Ollama model
```

### ğŸ“Œ Example Output
```
ğŸ”· OpenAI Response: Simulated response from OpenAI for: Hello
ğŸŸ£ Gemini Response: Simulated response from Gemini for: Hello
ğŸŸ¡ HuggingFace Response: Generated text from local GPT-2
ğŸŸ¢ Ollama Response: Real output from tinyllama
âœ… Demo completed successfully
```

---

# âš™ï¸ Setup & Installation

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/atharvvv10/MultiAgents.git
cd MultiAgents/llm_project
```

### 2ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

---

# âš¡ Setup Ollama for Local LLMs

### Install Ollama (macOS/Linux/Windows)  
â¡ï¸ https://ollama.ai/download

### Pull a model (example: TinyLlama)
```bash
ollama pull tinyllama
```

---

# â–¶ï¸ Running the Demo

```bash
PYTHONPATH=. python demo/run_demo.py
```

Youâ€™ll see responses from:
- Simulated OpenAI  
- Simulated Gemini  
- Offline HuggingFace GPT-2  
- Local Ollama model  

---

# ğŸ“ Agent Capabilities Summary

| Agent Type | Mode | Description |
|------------|------|-------------|
| **OpenAIAgent** | ğŸŸ© Offline | Simulated OpenAI responses |
| **GeminiAgent** | ğŸŸ© Offline | Simulated Gemini responses |
| **HuggingFaceAgent** | ğŸŸ¨ Offline | Runs GPT-2 or any HF model |
| **OllamaAgent** | âš¡ Local | Real model inference via `ollama` |

---

# ğŸ§© How to Add a New Agent

1. Open `src/agent_clients.py`  
2. Create a new class:
```python
class MyNewAgent:
    async def generate_response(self, text):
        ...
```
3. Add the agent to `run_demo.py`  
4. Write tests under `tests/`

The framework is intentionally simple & extendable.

---

# ğŸš€ Contributing

We welcome contributions in:
- New agent types  
- Better demos  
- Test improvements  
- Integrating more local models  

### Steps:
1. Fork the repo  
2. Create a feature branch  
3. Add your improvements  
4. Submit a pull request  
